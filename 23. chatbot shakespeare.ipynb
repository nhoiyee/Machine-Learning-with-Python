{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40ca30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b72d6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download and preprocess shakespeare text\n",
    "URL='https://gist.githubusercontent.com/CarineRam/817c25781a9ca8dc3370a190e31ab5e5/raw/ff02e4b6b3715295143846aaa896cc89f408cd67/gistfile1.txt'\n",
    "response = requests.get(URL)\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b444ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create character to integer mapping and vice versa\n",
    "chars= sorted(set(text))\n",
    "vocab_size= len(chars)\n",
    "char_to_int ={ch: i for i, ch in enumerate(chars)} #abc => {'a':0, 'b':1, 'c':2}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)} #abc => {0:'a', 1:'b', 2:'c'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1914ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode and decode functions\n",
    "def encode(s):\n",
    "    return [char_to_int[c] for c in s if c in char_to_int]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62cf6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(indices):\n",
    "    return ''.join([int_to_char[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8177bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create batches of data\n",
    "def get_batch(split, batch_size=32, block_size=128):\n",
    "\n",
    "    n=len(text)\n",
    "    split_idx= int(0.5* n)\n",
    "    if split== 'train':\n",
    "        data= text [:split_idx]\n",
    "    else:\n",
    "        data= text [split_idx:]\n",
    "    ix= torch.randint(len(data) - block_size-1, (batch_size,))\n",
    "    x_batch = torch.stack([torch.tensor(encode(data[i:i+block_size]), dtype=torch.long) for i in ix])\n",
    "\n",
    "    y_batch = torch.stack([torch.tensor(encode(data[i+1:i+block_size+1]), dtype=torch.long) for i in ix])\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96161d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model architecture\n",
    "vocab_size = 50257\n",
    "n_emb= 32\n",
    "block_size= 128\n",
    "head_size= 8\n",
    "num_heads = 4\n",
    "num_blocks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3389eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#single attention head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size)\n",
    "        self.query = nn.Linear(n_emb, head_size)\n",
    "        self.value = nn.Linear(n_emb, head_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        weights = (q @ k.transpose(-2, -1)) / (C ** 0.5)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        return weights @ v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb803c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple attention heads\n",
    "'''\n",
    "class MultipleAttention(nn.Module):\n",
    "    def __init__ (self, head_size, num_heads):\n",
    "        super().__init__ ()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range (head_heads)])\n",
    "        self.proj= nn.Linear(n_emb, n_emb)\n",
    "        self.dropout= nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out=torch.cat([h(v) for h in self.heads], dim=1)\n",
    "        return self.dropout(self.proj(out))\n",
    "'''\n",
    "\n",
    "    \n",
    "class MultipleAttention(nn.Module):\n",
    "    def __init__(self, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])  # Fix here\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # Use `x` instead of `v`\n",
    "        return self.dropout(self.proj(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58b2a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wise feedforward network\n",
    "   \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),\n",
    "            nn.ReLU(),  # Fixed typo\n",
    "            nn.Linear(4 * n_emb, n_emb),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bbdd761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module): #continue from before\n",
    "    def __init__ (self, n_emb, num_heads):\n",
    "        super().__init__ ()\n",
    "        head_size = n_emb// num_heads\n",
    "        self.sa= MultipleAttention(head_size, num_heads)\n",
    "        self.ff= FeedForward(n_emb)\n",
    "        self.ln1= nn.LayerNorm(n_emb)\n",
    "        self.ln2= nn.LayerNorm(n_emb)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x +self.sa(self.ln1(x))\n",
    "        x = x +self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b5cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module): #mine\n",
    "    def __init__ (self):\n",
    "        super().__init__ ()\n",
    "        self.token_emb= nn.Embedding(vocab_size, n_emb)\n",
    "        self.pos_emb= nn.Embedding(block_size, n_emb)\n",
    "        self.blocks= nn.Sequential(*[Block(n_emb, num_heads) for _ in range(num_blocks)])\n",
    "        self.ln_f= nn.LayerNorm(n_emb)\n",
    "        self.head =nn.Linear(n_emb, vocab_size)\n",
    "        \n",
    "       \n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        B, T = x.shape\n",
    "        tok_emb=self.token_emb(x)\n",
    "        pos_emb= self.pos_emb(torch.arange(T, device= x.device))\n",
    "        x= tok_emb+ pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x= self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        if y is None:\n",
    "            loss= None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view( B*T,vocab_size), y.view(B*T))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a37e6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, n_emb, block_size, num_heads, num_blocks):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, n_emb)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_emb)\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, num_heads) for _ in range(num_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(n_emb)\n",
    "        self.head = nn.Linear(n_emb, vocab_size)\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_emb(x)\n",
    "        pos_emb = self.pos_emb(torch.arange(T, device=x.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(B * T, vocab_size), y.view(B * T))\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113ffb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 10.938119888305664\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define model parameters\n",
    "vocab_size = 50257\n",
    "n_emb = 256\n",
    "block_size = 128\n",
    "num_heads = 8\n",
    "num_blocks = 6\n",
    "\n",
    "# Instantiate the model\n",
    "model = TextGenerator(vocab_size, n_emb, block_size, num_heads, num_blocks).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    for step in range(1000):\n",
    "        x, y = get_batch('train')  # Assuming `get_batch` is defined elsewhere\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            print(f'Step {step}, Loss: {loss.item()}')\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96cd6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate text\n",
    "model.eval()\n",
    "start_seq= 'the cat is cute'\n",
    "x = torch.tensor(encode(start_seq), dtype= torch.long).unsqueeze(0).to(device) #(1, T)\n",
    "generated = model.generate(x, max_new_tokens=50)\n",
    "print(decode(generated[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214b8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
